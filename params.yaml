data_general:
  log_dir: "logs/"

data_structure_raw:
  raw_dataset_path: "data/working_dataset.csv" 
  image_dir: "data/raw/"
  raw_dataset_dir: "data/raw_per_classes/" 

data_ingestion: # => OCR
  docker_service_ocr: ocr-service # /!\ à synchroniser avec docker-compose.yaml
  port_ocr: 8901
  route_ocr: txt/blocks-words
  ocr_endpoint: "http://${data_ingestion.docker_service_ocr}:${data_ingestion.port_ocr}/${data_ingestion.route_ocr}"
      # => endpoint_ocr
  ocr_text_dir: "data/ocr_raw_per_classes/"
  
data_cleaning: # => clean_text
  docker_service_clean_text: clean-service # /!\ à synchroniser avec docker-compose.yaml
  port_clean_text: 8903
  route_clean_text: clean
  clean_endpoint: "http://${data_cleaning.docker_service_clean_text}/clean"
      # => endpoint_clean_text
  cleaned_datasets_dir: "data/cleaned_per_classes/"

data_ETL:
  docker_service_ETL: etl-service
  port_ETL: 8907
  route_ETL_ingest_all: ingest
  route_ETL_clean_all: clean
  endpoint_ETL_ingest_all: "http://${data_ETL.docker_service_ETL}/${data_ETL.route_ETL_ingest_all}"
  endpoint_ETL_clean_all: "http://${data_ETL.docker_service_ETL}/${data_ETL.route_ETL_clean_all}"

data_preprocessing:
  docker_service_preprocessing: preprocessing-service
  port_preprocessing: 8904
  route_preprocessing: process
  endpoint_preprocessing: "http://${data_preprocessing.docker_service_preprocessing}/${data_preprocessing.route_preprocessing}"
  cleaned_datasets_dir: "data/cleaned_per_classes"
  models_dir: "models"
  vectorizer_dir: "${data_preprocessing.models_dir}/vectorizers"
  vectorizer_file : "tfidf_vectorizer.joblib"
  tfidf_vectorizer_path: "${data_preprocessing.vectorizer_dir}/${data_preprocessing.vectorizer_file}"
  preprocessed_data_dir: "data/processed"
  train_data_dir: "data/processed/train"
  test_data_dir: "data/processed/test"
  X_train_file: "X_train.joblib"
  X_train_path: "${data_preprocessing.train_data_dir}/${data_preprocessing.X_train_file}"
  y_train_file: "y_train.joblib"
  y_train_path: "${data_preprocessing.train_data_dir}/${data_preprocessing.y_train_file}"
  X_test_file: "X_test.joblib"
  X_test_path: "${data_preprocessing.test_data_dir}/${data_preprocessing.X_test_file}"
  y_test_file: "y_test.joblib"
  y_test_path: "${data_preprocessing.test_data_dir}/${data_preprocessing.y_test_file}"

model_train:
  docker_service_train: train-service
  port_train: 8905
  route_train: train
  endpoint_train: "http://${model_train.docker_service_train}/${model_train.route_train}"
  model_train_dir: "models/train"
  model_train_file: "ovrc.joblib"
  model_train_path: "${model_train.model_train_dir}/${model_train.model_train_file}"

model_eval:
  docker_service_eval: eval-service
  port_eval: 8906
  route_eval: eval
  endpoint_eval: "http://${model_eval.docker_service_eval}/${model_eval.route_eval}"
  metrics_dir: "metrics"
  metrics_score_file: "scores.json"
  metrics_confusion_matrix_file: "confusion_matrix.json"
  model_path: "${model_train.model_train_path}"

label_encoder_mapping:
  facture: 0
  id_pieces: 1
  resume: 2

mlflow:
  tracking_uri: "NOT_USED_FOR_DAGSHUB"
  experiment_id: "BAG_OF_WORDS"
  model_name: "BAG_OF_WORDS_MODEL"
  model_version: "1.0"